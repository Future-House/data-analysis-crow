{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "sys.path.append(\"/Users/ludovicomitchener/Desktop/repos/data-analysis-crow/src\")\n",
    "import hashlib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "from ldp.agent import AgentConfig\n",
    "from ldp.alg.rollout import RolloutManager\n",
    "from ldp.data_structures import Trajectory, Transition\n",
    "\n",
    "from fhda.data_analysis_env import DataAnalysisEnv\n",
    "from fhda.notebook_env import NBEnvironment\n",
    "from fhda.utils import NBLanguage\n",
    "from fhda import prompts\n",
    "import fhda.config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_analysis_env(\n",
    "    query: str, dataset: Path, language: NBLanguage = NBLanguage.PYTHON\n",
    "):\n",
    "    # Hash the task to get a unique identifier\n",
    "    task_hash = hashlib.sha256(query.encode()).hexdigest()\n",
    "    trajectory_path = (\n",
    "        Path(os.path.abspath(\"tmp_results_dir\")) / f\"{task_hash}-{time.time()}\"\n",
    "    )\n",
    "    trajectory_path.mkdir(parents=True, exist_ok=True)\n",
    "    nb_path = trajectory_path / NBEnvironment.NOTEBOOK_NAME\n",
    "    # Copy task data to trajectory path\n",
    "    if dataset.is_dir():\n",
    "        for item in dataset.iterdir():\n",
    "            if item.is_file():\n",
    "                shutil.copy2(item, trajectory_path)\n",
    "            elif item.is_dir():\n",
    "                shutil.copytree(item, trajectory_path / item.name, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(dataset, trajectory_path)\n",
    "    # Augment incoming task with CoT instructions\n",
    "    augmented_task = f\"\"\"\\\n",
    "    Here is the user query to address:\n",
    "\n",
    "\n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "\n",
    "    {prompts.CHAIN_OF_THOUGHT_AGNOSTIC.format(language=language.name)}\n",
    "    {prompts.GENERAL_NOTEBOOK_GUIDELINES.format(language=language.name)}\"\"\"\n",
    "\n",
    "    if language == NBLanguage.R:\n",
    "        augmented_task += f\"\\n{prompts.R_SPECIFIC_GUIDELINES}\"\n",
    "\n",
    "    dae = DataAnalysisEnv(\n",
    "        problem_id=f\"data-analysis-task-{task_hash}\",\n",
    "        problem=augmented_task,\n",
    "        eval_mode=None,\n",
    "        nb_path=nb_path,\n",
    "        work_dir=trajectory_path,\n",
    "        language=language,\n",
    "        system_prompt=prompts.CAPSULE_SYSTEM_PROMPT_QUERY,\n",
    "        use_tmp_work_dir=False,\n",
    "        run_notebook_on_edit=True if cfg.USE_DOCKER else False,\n",
    "    )\n",
    "    return dae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENT CONFIGURATION\n",
    "\n",
    "# Set your API keys\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# If using docker, be sure to pull the image from docker hub first\n",
    "# docker pull futurehouse/bixbench:aviary-notebook-env\n",
    "# This image includes many bioinformatics and data science packages\n",
    "cfg.USE_DOCKER = False\n",
    "# This can be R or PYTHON in Docker or with a local kernel if you have R installed\n",
    "LANGUAGE = NBLanguage.PYTHON\n",
    "MAX_STEPS = 3\n",
    "MODEL_NAME = \"claude-3-7-sonnet-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVIARY ROLLOUT\n",
    "# This folder only contains a single csv file on animal brain size and body mass from here:\n",
    "# https://animaltraits.org/\n",
    "# However, it could contain many files including nested folders\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Setting up data analysis environment\")\n",
    "\n",
    "dataset = Path(\"datasets/brain_size_data.csv\")\n",
    "query = \"Analyze the dataset and give me an in depth analysis using pretty plots. I am particularly interested in crows.\"\n",
    "environment = setup_data_analysis_env(query, dataset, LANGUAGE)\n",
    "\n",
    "agent = AgentConfig(\n",
    "    agent_type=\"ReActAgent\",\n",
    "    agent_kwargs={\n",
    "        \"llm_model\": {\n",
    "            \"parallel_tool_calls\": False,\n",
    "            \"num_retries\": 3,\n",
    "            \"temperature\": 1.0,\n",
    "            \"name\": MODEL_NAME,\n",
    "        },\n",
    "        \"hide_old_env_states\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "agent = agent.construct_agent()\n",
    "rollout = RolloutManager(agent=agent)\n",
    "\n",
    "# You can see the notebook updating live in the tmp_results_dir folder\n",
    "result = await rollout.sample_trajectories(\n",
    "    environments=[environment], max_steps=MAX_STEPS\n",
    ")\n",
    "\n",
    "print(\"Trajectory completed! Final notebook available at: \\n\", environment.nb_path)\n",
    "print(f\"Final agent answer:\\n{environment.state.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECT THE RESULT\n",
    "trajectory = result[0]\n",
    "# You can inspect each step in the trajectory and see what the agent's reasoning was,\n",
    "# what tool it called, and what the observation was\n",
    "for c, step in enumerate(trajectory.steps):\n",
    "    print(f\"Timestep {c}\")\n",
    "    print(f\"Done: {step.done}\")\n",
    "    print(\"Agent Reasoning:\")\n",
    "    for message in step.agent_state.messages:\n",
    "        if message.content:\n",
    "            print(f\"Message: {message.content[:200]} [Truncated]\")\n",
    "    # print(f\"Observation: {step.observation[:200]} [Truncated]\")\n",
    "    print(f\"Action: {step.action.value}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VANILLA ROLLOUT - this is a simple version of the what the rollout Manager does\n",
    "dataset_folder = Path(\"dataset\")\n",
    "query = \"Analyze the dataset and give me an in depth analysis using pretty plots. I am particularly interested in crows.\"\n",
    "environment = setup_data_analysis_env(query, dataset_folder)\n",
    "\n",
    "obs, tools = await environment.reset()\n",
    "agent_state = await agent.init_state(tools)\n",
    "trajectory = Trajectory()\n",
    "max_steps = 10\n",
    "for timestep in range(max_steps):\n",
    "    action, next_agent_state, value = await agent.get_asv(agent_state, obs)\n",
    "    next_obs, reward, done, trunc = await environment.step(action.value)\n",
    "    # Create the transition object\n",
    "    transition = Transition(\n",
    "        timestep=timestep,\n",
    "        agent_state=agent_state,\n",
    "        next_agent_state=next_agent_state,\n",
    "        observation=obs,\n",
    "        next_observation=next_obs,\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        done=done,\n",
    "        truncated=trunc,\n",
    "        value=value,\n",
    "    )\n",
    "    # Update steps by creating a new list with the additional transition\n",
    "    trajectory.steps = [*trajectory.steps, transition]\n",
    "    if done or trunc:\n",
    "        break\n",
    "\n",
    "    agent_state = next_agent_state\n",
    "    obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLATFORM ROLLOUT\n",
    "\n",
    "from futurehouse_client import FutureHouseClient\n",
    "from futurehouse_client.models import Stage, TaskRequest, RuntimeConfig\n",
    "from futurehouse_client.models.app import AuthType\n",
    "\n",
    "# CONFIGURATION\n",
    "CROW_STAGE = Stage.PROD\n",
    "API_KEY = \"\"\n",
    "JOB_NAME = \"job-futurehouse-data-analysis-crow-high\"\n",
    "MAX_STEPS = 25\n",
    "LANGUAGE = \"R\"\n",
    "DATA_GCS_LOCATION = \"bixbench_data/CapsuleFolder-1d54e4a7-8b0f-4224-bd31-efcfded0d46c\"\n",
    "\n",
    "\n",
    "client = FutureHouseClient(\n",
    "    stage=CROW_STAGE,\n",
    "    auth_type=AuthType.API_KEY,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "task = f\"\"\"\\\n",
    "Here is the user query to address:\n",
    "\n",
    "\n",
    "<query>\n",
    "Make a discovery using this dataset.\n",
    "</query>\n",
    "\n",
    "{prompts.CHAIN_OF_THOUGHT_AGNOSTIC.format(language=LANGUAGE)}\n",
    "{prompts.GENERAL_NOTEBOOK_GUIDELINES.format(language=LANGUAGE)}\"\"\"\n",
    "\n",
    "job_data = TaskRequest(\n",
    "    name=JOB_NAME,\n",
    "    query=task,\n",
    "    runtime_config=RuntimeConfig(\n",
    "        max_steps=MAX_STEPS,\n",
    "        upload_id=DATA_GCS_LOCATION,  # This is just an example dataset\n",
    "        environment_config={\n",
    "            \"run_notebook_on_edit\": False,\n",
    "            \"eval\": True,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        # timeout=600,\n",
    "    ),\n",
    ")\n",
    "job_id = client.create_task(job_data)\n",
    "status = \"in progress\"\n",
    "while status == \"in progress\":\n",
    "    print(\"Waiting for task to complete... checking again in 15 seconds\")\n",
    "    time.sleep(15)\n",
    "    status = client.get_task(job_id).status\n",
    "\n",
    "job_result = client.get_task(job_id, verbose=True)\n",
    "answer = job_result.environment_frame[\"state\"][\"state\"][\"answer\"]\n",
    "print(\n",
    "    f\"Task completed, the full analysis is available at:https://platform.futurehouse.org/trajectories/{job_id}\\n Agent answer: {answer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also view the notebook locally by saving it to a directory of your choice\n",
    "# Define the path where you want to save the notebook\n",
    "notebook_path = \"output/analysis_notebook.ipynb\"\n",
    "\n",
    "os.makedirs(os.path.dirname(notebook_path), exist_ok=True)\n",
    "notebook_content = job_result.environment_frame[\"state\"][\"state\"][\"nb_state\"]\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    json.dump(notebook_content, f, indent=2)\n",
    "\n",
    "print(f\"Notebook saved to {os.path.abspath(notebook_path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
